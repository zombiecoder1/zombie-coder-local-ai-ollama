# ğŸ§Ÿ ZombieCoder Local AI - Ollama Compatible

**à¦¯à§‡à¦–à¦¾à¦¨à§‡ à¦•à§‹à¦¡ à¦“ à¦•à¦¥à¦¾ à¦¬à¦²à§‡** | Where Code Meets Conversation

A lightweight, production-ready local AI framework with Ollama-compatible API endpoints. Run powerful language models locally with minimal setup.

[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](./test)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## âœ¨ Features

### ğŸš€ Core Capabilities
- **Ollama-Compatible API** - Drop-in replacement for Ollama endpoints
- **Multiple Model Support** - GGUF format models (TinyLlama, Phi-2, etc.)
- **Session Management** - Track conversation context across requests
- **Model Lifecycle** - Lazy loading, auto-unload, idle detection
- **Download Manager** - Direct HuggingFace Hub integration
- **Runtime Orchestrator** - llama.cpp backend with process management

### ğŸ¯ Key Benefits
- âœ… **100% Local** - No cloud dependencies, complete privacy
- âœ… **Lightweight** - ~15MB framework, models load on-demand
- âœ… **Fast** - 14+ tokens/sec on CPU, faster with GPU
- âœ… **Easy Setup** - Python + llama.cpp, no complex dependencies
- âœ… **Production Ready** - Full test suite, error handling, logging

---

## ğŸ–¥ï¸ System Requirements

### Minimum
- **OS:** Windows 10/11, Linux, macOS
- **RAM:** 8 GB (16 GB recommended)
- **Storage:** 10 GB free space
- **Python:** 3.8 or higher

### Recommended
- **RAM:** 16 GB+
- **GPU:** CUDA-compatible (optional, for 10x+ speed)
- **Storage:** 20 GB+ (for multiple models)

---

## ğŸ“¦ Installation

### 1. Clone Repository
```bash
git clone https://github.com/zombiecoder1/zombie-coder-local-ai-ollama.git
cd zombie-coder-local-ai-ollama
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Download llama.cpp Runtime
The framework uses llama.cpp for inference. Download from:
- **Windows:** [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases)
- **Linux/Mac:** Build from source or use pre-built binaries

Place `server.exe` (Windows) or `server` (Linux/Mac) in:
```
C:\model\config\llama.cpp\server.exe  (Windows)
~/model/config/llama.cpp/server       (Linux/Mac)
```

### 4. Start Server
```bash
python model_server.py
```

Server starts at: http://localhost:8155

---

## ğŸš€ Quick Start

### Download a Model
```bash
# Using API
curl -X POST http://localhost:8155/download/start \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "tinyllama-gguf",
    "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
  }'

# Check download progress
curl http://localhost:8155/download/status/tinyllama-gguf
```

### Load Model
```bash
curl -X POST http://localhost:8155/runtime/load/tinyllama-gguf?threads=4
```

### Generate Text
```bash
curl -X POST http://localhost:8155/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama-gguf",
    "prompt": "What is the meaning of life?"
  }'
```

### Unload Model
```bash
curl -X POST http://localhost:8155/runtime/unload/tinyllama-gguf
```

---

## ğŸ“š API Documentation

### Ollama-Compatible Endpoints

#### List Models
```bash
GET /api/tags
```

#### Generate Text
```bash
POST /api/generate
{
  "model": "model-name",
  "prompt": "Your prompt here",
  "stream": false
}
```

### Extended Endpoints

#### System Info
```bash
GET /health              # Server health
GET /system/info         # Hardware detection
GET /models/installed    # List local models
GET /models/available    # Recommended models
```

#### Runtime Control
```bash
GET  /runtime/status           # Current state
POST /runtime/load/{model}     # Load model
POST /runtime/unload/{model}   # Unload model
```

#### Session Management
```bash
POST /api/session/start              # Create session
GET  /api/session/status/{id}        # Check session
POST /api/session/end/{id}           # End session
```

#### Download Manager
```bash
POST /download/start                 # Start download
GET  /download/status/{model}        # Check progress
POST /download/cancel/{model}        # Cancel download
```

Full API documentation: [API.md](docs/API.md)

---

## ğŸ§ª Testing

### Run All Tests
```bash
cd test
python run_core_tests.py
```

### Individual Tests
```bash
python test/01_preflight_check.py       # System health
python test/02_model_lifecycle.py       # Model operations
python test/03_api_standard_check.py    # API compatibility
python test/04_ui_data_integrity.py     # Data validation
python test/05_integrated_session_test.py  # Session management
```

**Latest Test Results:** âœ… 5/5 Core Tests Passing (100%)

---

## ğŸ“Š Performance

### Benchmarks (TinyLlama 1.1B, Q2_K)

| Metric | CPU (4 cores) | GPU (CUDA) |
|--------|---------------|------------|
| Load Time | 3-4 seconds | 2-3 seconds |
| Tokens/sec | 14 tokens/s | 150+ tokens/s |
| Memory Usage | 2-3 GB | 1-2 GB |
| First Token | ~60ms | ~20ms |

**System:** Intel i5-4590, 16GB RAM, Windows 10  
**GPU:** Intel HD Graphics 4400 (integrated)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FastAPI Server (Port 8155)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Ollama   â”‚  â”‚ Download â”‚  â”‚ Session  â”‚ â”‚
â”‚  â”‚ API      â”‚  â”‚ Manager  â”‚  â”‚ Manager  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Runtime Orchestrator                 â”‚
â”‚  (Model Loading, Process Management)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         llama.cpp Server (Dynamic)          â”‚
â”‚              (Port 8080+)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Components:**
- **API Gateway:** FastAPI-based REST API
- **Runtime Orchestrator:** Model lifecycle management
- **Download Manager:** HuggingFace Hub integration
- **Session Manager:** Conversation state tracking
- **Model Registry:** Local model database

---

## ğŸ”§ Configuration

### Environment Variables
```bash
MODEL_SERVER_PORT=8155          # API server port
MODELS_DIR=/path/to/models      # Models directory
HUGGINGFACE_HUB_TOKEN=your_token  # HF token (for private models)
```

### Model Recommendations

| System | Recommended Model | Quantization | RAM | Speed |
|--------|------------------|--------------|-----|-------|
| Entry (8GB) | TinyLlama 1.1B | Q2_K | 2GB | 10-15 t/s |
| Good (16GB) | Phi-2 2.7B | Q4_K_M | 4GB | 8-12 t/s |
| High (32GB) | Llama-2 7B | Q5_K_M | 8GB | 5-10 t/s |

---

## ğŸ“ Examples

### Python Client
```python
import requests

# Load model
response = requests.post(
    "http://localhost:8155/runtime/load/tinyllama-gguf",
    params={"threads": 4}
)

# Generate text
response = requests.post(
    "http://localhost:8155/api/generate",
    json={
        "model": "tinyllama-gguf",
        "prompt": "Explain quantum computing in simple terms"
    }
)

print(response.json())
```

### JavaScript Client
```javascript
// Generate text
const response = await fetch('http://localhost:8155/api/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'tinyllama-gguf',
    prompt: 'What is AI?'
  })
});

const data = await response.json();
console.log(data.runtime_response.content);
```

More examples: [examples/](examples/)

---

## ğŸ› ï¸ Troubleshooting

### Model Not Loading
```bash
# Check runtime config
curl http://localhost:8155/runtime/config

# Verify model exists
curl http://localhost:8155/models/installed
```

### Slow Performance
- Use quantized models (Q2_K, Q4_K)
- Enable GPU acceleration (if available)
- Increase thread count: `?threads=8`

### Out of Memory
- Use smaller quantization (Q2_K instead of Q8_0)
- Use smaller model (TinyLlama instead of Phi-2)
- Close other applications

Common issues: [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)

---

## ğŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md)

### Development Setup
```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests
cd test && python run_all_tests.py

# Check code style
black . --check
pylint model_server.py
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

---

## ğŸ‘¥ Credits

**Created by:** Sahon Srabon  
**Organization:** Developer Zone  
**Website:** https://zombiecoder.my.id/  
**Contact:** infi@zombiecoder.my.id

### Technologies Used
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Inference engine
- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [HuggingFace Hub](https://huggingface.co/) - Model repository
- [Ollama](https://ollama.ai/) - API inspiration

---

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=zombiecoder1/zombie-coder-local-ai-ollama&type=Date)](https://star-history.com/#zombiecoder1/zombie-coder-local-ai-ollama&Date)

---

## ğŸ“ Support

- **Issues:** [GitHub Issues](https://github.com/zombiecoder1/zombie-coder-local-ai-ollama/issues)
- **Email:** infi@zombiecoder.my.id
- **Phone:** +880 1323-626282
- **Address:** 235 south pirarbag, Amtala Bazar, Mirpur - 60 feet

---

**Made with â¤ï¸ by ZombieCoder**  
*Building AI tools that respect privacy and run anywhere*

