# ğŸ”— ZombieCoder Systems Integration Guide

à¦†à¦ªà¦¨à¦¾à¦° à¦•à¦¾à¦›à§‡ à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨à§‡ **à¦¦à§à¦Ÿà¦¿ à¦¸à¦¿à¦¸à§à¦Ÿà§‡à¦®** à¦šà¦¾à¦²à§ à¦†à¦›à§‡à¥¤ à¦šà¦²à§à¦¨ à¦¦à§‡à¦–à¦¿ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦à¦—à§à¦²à§‹ à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦•à¦¾à¦œ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

---

## ğŸ“Š à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦¸à¦¿à¦¸à§à¦Ÿà§‡à¦® à¦¸à§à¦Ÿà§à¦¯à¦¾à¦Ÿà¦¾à¦¸

### ğŸŸ¢ System 1: ZombieCoder Advanced System
**Location:** `C:\zombiecoder_advanced_system`

#### Active Services (4/5):
| Service | Port | Status | Purpose |
|---------|------|--------|---------|
| API Gateway | 8000 | âœ… Running | Main API entry point |
| Memory Service | 8001 | âœ… Running | Conversation memory |
| Multiprocessing Service | 8002 | âœ… Running | Parallel processing |
| SSL Server | 3443 | âŒ Not responding | HTTPS endpoint |

**Features:**
- Bengali Language Support âœ“
- SSL Certificates Generated âœ“
- Microservices Architecture âœ“

**Issues:**
- âŒ Ollama models (deepseek-coder:1.3b, llama2:7b) **NOT INSTALLED**
- âŒ SSL Server not responding

---

### ğŸŸ¢ System 2: ZombieCoder Local AI Framework
**Location:** `C:\model`  
**Port:** 8155

#### Installed Models (3):
| Model | Size | Format | Status | Working |
|-------|------|--------|--------|---------|
| **tinyllama-gguf** | 8.1 GB | GGUF | Ready | âœ… **YES** |
| phi-2 | 5.3 GB | Safetensors | Detected | âŒ No (needs conversion) |
| tinyllama | 2.1 GB | Safetensors | Detected | âŒ No (needs conversion) |

**Features:**
- âœ… GGUF Model Support (Working)
- âœ… Ollama-compatible API
- âœ… Text Generation (~14 tokens/sec)
- âœ… Session Management
- âœ… Download Manager
- âœ… Full Test Suite (100% passing)

---

## ğŸ”§ à¦•à¦¿ à¦•à¦¿ à¦•à¦¾à¦œ à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à¦¬à§‡à¦¨

### 1. ğŸ¤– AI Text Generation (Local AI - Port 8155)

#### Model Load à¦•à¦°à§à¦¨:
```bash
curl -X POST http://localhost:8155/runtime/load/tinyllama-gguf?threads=4
```

#### Text Generate à¦•à¦°à§à¦¨:
```bash
curl -X POST http://localhost:8155/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama-gguf",
    "prompt": "à¦¬à¦¾à¦‚à¦²à¦¾ à¦­à¦¾à¦·à¦¾à¦¯à¦¼ AI à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à§«à§¦ à¦¶à¦¬à§à¦¦à§‡ à¦²à¦¿à¦–à§à¦¨"
  }'
```

#### Code Generation:
```bash
curl -X POST http://localhost:8155/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama-gguf",
    "prompt": "Write a Python function to calculate factorial"
  }'
```

---

### 2. ğŸ’¾ Memory Management (Advanced System - Port 8001)

#### Save Conversation:
```bash
curl -X POST http://localhost:8001/memory/save \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "user123",
    "message": "Hello, how are you?",
    "response": "I am fine, thank you!"
  }'
```

#### Retrieve Memory:
```bash
curl http://localhost:8001/memory/get/user123
```

---

### 3. âš¡ Multiprocessing (Advanced System - Port 8002)

#### Parallel Task Processing:
```bash
curl -X POST http://localhost:8002/process/parallel \
  -H "Content-Type: application/json" \
  -d '{
    "tasks": [
      {"type": "compute", "data": "task1"},
      {"type": "compute", "data": "task2"},
      {"type": "compute", "data": "task3"}
    ]
  }'
```

---

### 4. ğŸ”— Integration: à¦¦à§à¦Ÿà¦¿ à¦¸à¦¿à¦¸à§à¦Ÿà§‡à¦® à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°

#### Example: AI + Memory Integration

**Step 1:** Generate AI response (Local AI)
```bash
RESPONSE=$(curl -X POST http://localhost:8155/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"tinyllama-gguf","prompt":"What is Python?"}' \
  | jq -r '.runtime_response.content')
```

**Step 2:** Save to Memory (Advanced System)
```bash
curl -X POST http://localhost:8001/memory/save \
  -H "Content-Type: application/json" \
  -d "{
    \"session_id\": \"user123\",
    \"message\": \"What is Python?\",
    \"response\": \"$RESPONSE\"
  }"
```

---

## ğŸš€ Recommended Use Cases

### 1. **Chatbot with Memory**
```
User â†’ Advanced System (8000) 
     â†’ Local AI (8155) [Generate Response]
     â†’ Memory Service (8001) [Save Context]
     â†’ Response to User
```

### 2. **Code Assistant**
```
Code Query â†’ Local AI (tinyllama-gguf)
          â†’ Generate Code
          â†’ Memory Service (Save Solution)
```

### 3. **Document Processing**
```
Documents â†’ Multiprocessing (8002) [Parallel Processing]
         â†’ Local AI (8155) [Summarization]
         â†’ Memory (8001) [Store Results]
```

### 4. **Bengali Language AI**
```
Bengali Query â†’ Local AI (8155) [Translation/Response]
             â†’ Memory (8001) [Context Tracking]
```

---

## âš™ï¸ Setup Missing Components

### Install Ollama Models (for Advanced System)
```bash
# Install deepseek-coder
ollama pull deepseek-coder:1.3b

# Install llama2
ollama pull llama2:7b
```

### Fix SSL Server
```bash
cd C:\zombiecoder_advanced_system
npm run ssl-server
```

---

## ğŸ”„ Integration Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ZombieCoder Advanced System         â”‚
â”‚            (Port 8000-8002)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   API    â”‚  â”‚  Memory  â”‚  â”‚ Multi-   â”‚ â”‚
â”‚  â”‚ Gateway  â”‚  â”‚ Service  â”‚  â”‚processingâ”‚ â”‚
â”‚  â”‚  :8000   â”‚  â”‚  :8001   â”‚  â”‚  :8002   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ Integration Layer
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ZombieCoder Local AI Framework         â”‚
â”‚              (Port 8155)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  GGUF    â”‚  â”‚ Ollama   â”‚  â”‚ Session  â”‚ â”‚
â”‚  â”‚ Models   â”‚  â”‚   API    â”‚  â”‚ Manager  â”‚ â”‚
â”‚  â”‚(TinyLlamaâ”‚  â”‚          â”‚  â”‚          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Example: Complete Workflow

### Bengali AI Assistant with Memory

```python
import requests
import json

# 1. Start session (Advanced System)
session = requests.post('http://localhost:8000/api/session/start').json()
session_id = session['session_id']

# 2. Load AI model (Local AI)
requests.post('http://localhost:8155/runtime/load/tinyllama-gguf?threads=4')

# 3. User asks question
user_query = "à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦¬à¦²à§à¦¨"

# 4. Generate response (Local AI)
ai_response = requests.post('http://localhost:8155/api/generate', json={
    "model": "tinyllama-gguf",
    "prompt": user_query
}).json()

response_text = ai_response['runtime_response']['content']

# 5. Save to memory (Advanced System)
requests.post('http://localhost:8001/memory/save', json={
    "session_id": session_id,
    "message": user_query,
    "response": response_text
})

# 6. Return response
print(f"AI: {response_text}")

# 7. Later: Retrieve conversation history
history = requests.get(f'http://localhost:8001/memory/get/{session_id}').json()
print(f"History: {history}")
```

---

## ğŸ¯ Performance Metrics

### Local AI (Port 8155)
- **Model:** tinyllama-gguf (Q2_K)
- **Speed:** ~14 tokens/second (CPU)
- **Load Time:** 3-4 seconds
- **Memory:** ~2-3 GB

### Advanced System
- **API Gateway:** <50ms latency
- **Memory Service:** <100ms query time
- **Multiprocessing:** Parallel execution

---

## âœ… What You Can Do RIGHT NOW

### 1. Text Generation âœ…
```bash
# Load model first
curl -X POST http://localhost:8155/runtime/load/tinyllama-gguf

# Generate text
curl -X POST http://localhost:8155/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"tinyllama-gguf","prompt":"Hello, world!"}'
```

### 2. Save Conversations âœ…
```bash
curl -X POST http://localhost:8001/memory/save \
  -H "Content-Type: application/json" \
  -d '{"session_id":"test","message":"Hi","response":"Hello!"}'
```

### 3. Parallel Processing âœ…
```bash
curl -X POST http://localhost:8002/process/parallel \
  -H "Content-Type: application/json" \
  -d '{"tasks":[{"id":1},{"id":2}]}'
```

---

## ğŸ”® Future Enhancements

### To Add:
1. âœ… **Working:** Local AI models (tinyllama-gguf)
2. â³ **Pending:** Ollama models (deepseek-coder, llama2)
3. â³ **Pending:** SSL Server fix
4. âœ… **Working:** Memory persistence
5. âœ… **Working:** Session management

### To Install:
```bash
# Install missing Ollama models
ollama pull deepseek-coder:1.3b
ollama pull llama2:7b

# Then they'll be available via Advanced System
```

---

## ğŸ“ Quick Reference

### Port Summary
- **8000** - API Gateway (Advanced)
- **8001** - Memory Service (Advanced)
- **8002** - Multiprocessing (Advanced)
- **8155** - Local AI Framework
- **3443** - SSL Server (not working)

### Working Models
- **tinyllama-gguf** (8.1 GB) - âœ… Text generation
- **deepseek-coder:1.3b** - âŒ Not installed
- **llama2:7b** - âŒ Not installed

### Integration Points
1. Advanced System â†’ Local AI (for AI responses)
2. Local AI â†’ Memory Service (for context)
3. Memory Service â†’ Multiprocessing (for batch tasks)

---

**ğŸ‰ Summary:**
- âœ… **2 Systems Running**
- âœ… **1 Working AI Model** (tinyllama-gguf)
- âœ… **4 Active Services**
- âœ… **Full Integration Possible**

**Next Step:** Install Ollama models for full functionality!

