# ğŸ§Ÿ ZombieCoder Local AI Framework

**à¦¯à§‡à¦–à¦¾à¦¨à§‡ à¦•à§‹à¦¡ à¦“ à¦•à¦¥à¦¾ à¦¬à¦²à§‡** | A lightweight local AI framework for running language models locally

[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](./test)
[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## âœ¨ Features

### ğŸš€ Core Capabilities
- **Local AI Models** - Run GGUF format models (TinyLlama, Phi-2, etc.)
- **REST API** - FastAPI-based server with comprehensive endpoints
- **Session Management** - Track conversation context across requests
- **Model Lifecycle** - Lazy loading, auto-unload, idle detection
- **Download Manager** - Direct HuggingFace Hub integration
- **Runtime Orchestrator** - llama.cpp backend with process management

### ğŸ¯ Key Benefits
- âœ… **100% Local** - No cloud dependencies, complete privacy
- âœ… **Lightweight** - ~15MB framework, models load on-demand
- âœ… **Fast** - 14+ tokens/sec on CPU, faster with GPU
- âœ… **Easy Setup** - Python + llama.cpp, minimal dependencies
- âœ… **Production Ready** - Full test suite (100% passing), error handling, logging

---

## ğŸ–¥ï¸ System Requirements

### Minimum
- **OS:** Windows 10/11, Linux, macOS
- **RAM:** 8 GB (16 GB recommended)
- **Storage:** 10 GB free space
- **Python:** 3.8 or higher

### Recommended
- **RAM:** 16 GB+
- **GPU:** CUDA-compatible (optional, for 10x+ speed)
- **Storage:** 20 GB+ (for multiple models)

---

## ğŸ“¦ Installation

### 1. Clone Repository
```bash
git clone https://github.com/zombiecoder1/zombie-coder-local-ai-ollama.git
cd zombie-coder-local-ai-ollama
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Download llama.cpp Runtime
Download llama.cpp from [official releases](https://github.com/ggerganov/llama.cpp/releases)

Place binary in:
```
C:\model\config\llama.cpp\server.exe  (Windows)
~/model/config/llama.cpp/server       (Linux/Mac)
```

### 4. Start Server
```bash
python model_server.py
```

Server starts at: **http://localhost:8155**


---

## ğŸš€ Quick Start

### Download a Model
```bash
curl -X POST http://localhost:8155/download/start \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "tinyllama-gguf",
    "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
  }'

# Check progress
curl http://localhost:8155/download/status/tinyllama-gguf
```

### Load & Use Model
```bash
# Load model
curl -X POST http://localhost:8155/runtime/load/tinyllama-gguf?threads=4

# Generate text
curl -X POST http://localhost:8155/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama-gguf",
    "prompt": "Write a Python hello world program"
  }'

# Unload model
curl -X POST http://localhost:8155/runtime/unload/tinyllama-gguf
```

---

## ğŸ“š Documentation

Detailed documentation available in [`doc/`](./doc/) folder:

- **[API Documentation](./doc/README.md)** - Complete API reference
- **[System Integration](./doc/SYSTEMS_INTEGRATION.md)** - Integration guide
- **[Current Capabilities](./doc/CURRENT_CAPABILITIES.md)** - Features & limitations
- **[Verification Report](./doc/VERIFICATION_COMPLETE.md)** - Test results

---

## ğŸ§ª Testing

### Run All Tests
```bash
cd test
python run_core_tests.py
```

### Test Results
âœ… **5/5 Core Tests Passing (100%)**

- `01_preflight_check.py` - System health check
- `02_model_lifecycle.py` - Model operations
- `03_api_standard_check.py` - API compatibility
- `04_ui_data_integrity.py` - Data validation
- `05_integrated_session_test.py` - Session management

**Test Coverage:** See [`test/README.md`](./test/README.md)

---

## ğŸ“Š API Endpoints

### System Info
```bash
GET  /health              # Server health
GET  /system/info         # Hardware detection
GET  /models/installed    # List local models
GET  /models/available    # Recommended models
```

### Runtime Control
```bash
GET  /runtime/status           # Current state
POST /runtime/load/{model}     # Load model
POST /runtime/unload/{model}   # Unload model
GET  /runtime/config           # Configuration
```

### Text Generation
```bash
POST /api/generate             # Generate text
GET  /api/tags                 # List models
```

### Session Management
```bash
POST /api/session/start              # Create session
GET  /api/session/status/{id}        # Check session
POST /api/session/end/{id}           # End session
```

### Download Manager
```bash
POST /download/start                 # Start download
GET  /download/status/{model}        # Check progress
POST /download/cancel/{model}        # Cancel download
```

Full API docs: [`doc/README.md`](./doc/README.md)

---

## ğŸ“ˆ Performance

### Benchmarks (TinyLlama 1.1B, Q2_K)

| Metric | CPU (4 cores) | GPU (CUDA) |
|--------|---------------|------------|
| Load Time | 3-4 seconds | 2-3 seconds |
| Tokens/sec | 14 tokens/s | 150+ tokens/s |
| Memory Usage | 2-3 GB | 1-2 GB |
| First Token | ~60ms | ~20ms |

**Tested on:** Intel i5-4590, 16GB RAM, Windows 10

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Server (Port 8155)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Models â”‚  â”‚Downloadâ”‚  â”‚ Session  â”‚ â”‚
â”‚  â”‚ API    â”‚  â”‚Manager â”‚  â”‚ Manager  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Runtime Orchestrator             â”‚
â”‚  (Model Loading, Process Management)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      llama.cpp Server (Dynamic)         â”‚
â”‚           (Port 8080+)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Usage Examples

### Python Client
```python
import requests

# Load model
requests.post(
    "http://localhost:8155/runtime/load/tinyllama-gguf",
    params={"threads": 4}
)

# Generate text
response = requests.post(
    "http://localhost:8155/api/generate",
    json={
        "model": "tinyllama-gguf",
        "prompt": "Explain quantum computing"
    }
)

print(response.json()['runtime_response']['content'])
```

### JavaScript Client
```javascript
// Generate text
const response = await fetch('http://localhost:8155/api/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'tinyllama-gguf',
    prompt: 'What is AI?'
  })
});

const data = await response.json();
console.log(data.runtime_response.content);
```

More examples: [`examples/`](./examples/)

---

## ğŸ”§ Configuration

### Environment Variables
```bash
MODEL_SERVER_PORT=8155          # API server port
MODELS_DIR=/path/to/models      # Models directory
HUGGINGFACE_HUB_TOKEN=token     # HF token (optional)
```

### Recommended Models

| System RAM | Model | Quantization | Speed |
|------------|-------|--------------|-------|
| 8 GB | TinyLlama 1.1B | Q2_K | 10-15 t/s |
| 16 GB | Phi-2 2.7B | Q4_K_M | 8-12 t/s |
| 32 GB | Llama-2 7B | Q5_K_M | 5-10 t/s |

---

## ğŸ› ï¸ Troubleshooting

### Model Not Loading
```bash
# Check runtime config
curl http://localhost:8155/runtime/config

# Verify model exists
curl http://localhost:8155/models/installed
```

### Slow Performance
- Use quantized models (Q2_K, Q4_K)
- Increase thread count: `?threads=8`
- Enable GPU acceleration (if available)

### Out of Memory
- Use smaller quantization (Q2_K instead of Q8_0)
- Use smaller model (TinyLlama vs Phi-2)
- Close other applications

---

## ğŸ“ Project Structure

```
C:\model/
â”œâ”€â”€ doc/                    # ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md          # API reference
â”‚   â”œâ”€â”€ SYSTEMS_INTEGRATION.md
â”‚   â”œâ”€â”€ CURRENT_CAPABILITIES.md
â”‚   â””â”€â”€ VERIFICATION_COMPLETE.md
â”œâ”€â”€ test/                   # ğŸ§ª Test suite
â”‚   â”œâ”€â”€ 01_preflight_check.py
â”‚   â”œâ”€â”€ 02_model_lifecycle.py
â”‚   â”œâ”€â”€ 03_api_standard_check.py
â”‚   â”œâ”€â”€ 04_ui_data_integrity.py
â”‚   â”œâ”€â”€ 05_integrated_session_test.py
â”‚   â”œâ”€â”€ run_core_tests.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ static/                 # ğŸŒ Web UI
â”œâ”€â”€ models/                 # ğŸ¤– Downloaded models
â”œâ”€â”€ logs/                   # ğŸ“ Server logs
â”œâ”€â”€ data/                   # ğŸ’¾ Database
â”œâ”€â”€ model_server.py        # ğŸš€ Main server
â”œâ”€â”€ router.py              # ğŸ”„ Runtime orchestrator
â”œâ”€â”€ downloader.py          # â¬‡ï¸ Download manager
â”œâ”€â”€ db_manager.py          # ğŸ’¾ Model registry
â”œâ”€â”€ system_detector.py     # ğŸ–¥ï¸ Hardware detection
â””â”€â”€ requirements.txt       # ğŸ“¦ Dependencies
```

---

## ğŸ¤ Contributing

Contributions welcome! See [`CONTRIBUTING.md`](./CONTRIBUTING.md)

### Development Setup
```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests
cd test && python run_all_tests.py
```

---

## ğŸ“„ License

MIT License - see [`LICENSE`](./LICENSE)

---

## ğŸ‘¥ Credits

**Created by:** Sahon Srabon  
**Organization:** Developer Zone  
**Website:** https://zombiecoder.my.id/  
**Contact:** infi@zombiecoder.my.id

### Technologies
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Inference engine
- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [HuggingFace Hub](https://huggingface.co/) - Model repository

---

## ğŸ“ Support

- **GitHub Issues:** [Report issues](https://github.com/zombiecoder1/zombie-coder-local-ai-ollama/issues)
- **Email:** infi@zombiecoder.my.id
- **Phone:** +880 1323-626282

---

## ğŸ¯ Quick Links

- ğŸ“š [Full Documentation](./doc/)
- ğŸ§ª [Test Suite](./test/)
- ğŸŒ [Web UI](http://localhost:8155)
- ğŸ“Š [API Reference](./doc/README.md)
- âœ… [Test Results](./doc/VERIFICATION_COMPLETE.md)

---

**Made with â¤ï¸ by ZombieCoder**  
*Building AI tools that respect privacy and run anywhere*

