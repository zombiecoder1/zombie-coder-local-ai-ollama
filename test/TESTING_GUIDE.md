# üß™ ‡¶ü‡ßá‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶ó‡¶æ‡¶á‡¶° - Testing Guide

**ZombieCoder Local AI Framework**

‡¶è‡¶á ‡¶ó‡¶æ‡¶á‡¶° ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá testui.html ‡¶è‡¶¨‡¶Ç ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø test tools ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ test ‡¶ï‡¶∞‡¶¨‡ßá‡¶®‡•§

---

## üìã Table of Contents

1. [‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶∞‡¶æ](#‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞-‡¶ö‡¶æ‡¶≤‡ßÅ-‡¶ï‡¶∞‡¶æ)
2. [Test UI ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞](#test-ui-‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞)
3. [API Endpoints ‡¶ü‡ßá‡¶∏‡ßç‡¶ü](#api-endpoints-‡¶ü‡ßá‡¶∏‡ßç‡¶ü)
4. [Model Testing](#model-testing)
5. [Automated Tests](#automated-tests)

---

## üöÄ ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶∞‡¶æ

### Step 1: Server Start
```bash
# Terminal ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®
cd C:\model

# Server ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
python model_server.py
```

**Expected Output:**
```
INFO: Uvicorn running on http://0.0.0.0:8155
INFO: Started server process
INFO: Waiting for application startup
INFO: Application startup complete
```

### Step 2: Verify Server Running
```bash
# ‡¶®‡¶§‡ßÅ‡¶® terminal ‡¶ñ‡ßÅ‡¶≤‡ßá ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®
curl http://localhost:8155/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "service": "ZombieCoder Local AI Framework",
  "version": "0.1.0",
  "port": 8155
}
```

‚úÖ ‡¶Ø‡¶¶‡¶ø ‡¶è‡¶á response ‡¶™‡¶æ‡¶® = Server ‡¶∏‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶Ü‡¶õ‡ßá!

---

## üåê Test UI ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞

### Method 1: Direct File Open (‡¶∏‡¶π‡¶ú)

```bash
# Browser ‡¶è ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®
file:///C:/model/test/testui.html
```

### Method 2: Test Server ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá (Recommended)

```bash
# Test UI server ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®
cd C:\model\test
python testui.py
```

‡¶§‡¶æ‡¶∞‡¶™‡¶∞ browser ‡¶è ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®: **http://localhost:8080**

---

## üéØ Test UI Features

### 1. System Information Card
**‡¶Ø‡¶æ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá:**
- CPU model
- RAM size
- GPU information
- OS version
- System tier (entry_level/good/high_end)

**API:**
```bash
GET http://localhost:8155/system/info
```

**Test ‡¶ï‡¶∞‡ßÅ‡¶®:**
- "Refresh Server Data" button click ‡¶ï‡¶∞‡ßÅ‡¶®
- System info update ‡¶π‡¶¨‡ßá

---

### 2. Models Card

**‡¶Ø‡¶æ ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá:**
- **Installed Models:** ‡¶Ø‡ßá models download ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá
- **Available Models:** ‡¶Ø‡ßá models download ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá

**APIs:**
```bash
GET http://localhost:8155/models/installed
GET http://localhost:8155/models/available
```

**Test ‡¶ï‡¶∞‡ßÅ‡¶®:**
- Refresh ‡¶ï‡¶∞‡¶≤‡ßá model list update ‡¶π‡¶¨‡ßá
- Model chips click ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá

---

### 3. Chat Interface

**Features:**
- Message input box
- Send button
- Chat history
- Current model indicator

**‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá test ‡¶ï‡¶∞‡¶¨‡ßá‡¶®:**

#### Option A: Simulated (Default)
```
1. Message box ‡¶è ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®
2. Send click ‡¶ï‡¶∞‡ßÅ‡¶®
3. Simulated response ‡¶™‡¶æ‡¶¨‡ßá‡¶®
```

#### Option B: Real Model (Recommended)

**Step 1:** Model load ‡¶ï‡¶∞‡ßÅ‡¶®
```bash
curl -X POST http://localhost:8155/runtime/load/tinyllama-gguf?threads=4
```

**Step 2:** testui.html ‡¶è code uncomment ‡¶ï‡¶∞‡ßÅ‡¶®
- `sendMessage()` function ‡¶è ‡¶Ø‡¶æ‡¶®
- "REAL API CALL" section uncomment ‡¶ï‡¶∞‡ßÅ‡¶®
- "SIMULATED RESPONSE" section comment out ‡¶ï‡¶∞‡ßÅ‡¶®

**Step 3:** Chat ‡¶ï‡¶∞‡ßÅ‡¶®
```
1. Message box ‡¶è ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®: "Write Python hello world"
2. Send click ‡¶ï‡¶∞‡ßÅ‡¶®
3. Real model response ‡¶™‡¶æ‡¶¨‡ßá‡¶® 3-5 ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶°‡ßá
```

**API Call:**
```bash
POST http://localhost:8155/api/generate
Content-Type: application/json

{
  "model": "tinyllama-gguf",
  "prompt": "Your message here"
}
```

---

### 4. Auto Chat Questions

**Pre-defined questions:**
- Write a Python hello world program
- Explain machine learning in simple terms
- What is FastAPI and why use it?
- Create a function to calculate factorial
- Explain async/await in Python

**Test ‡¶ï‡¶∞‡ßÅ‡¶®:**
- ‡¶Ø‡ßá‡¶ï‡ßã‡¶®‡ßã button click ‡¶ï‡¶∞‡ßÅ‡¶®
- Question automatically chat ‡¶è ‡¶Ø‡¶æ‡¶¨‡ßá
- Response ‡¶™‡¶æ‡¶¨‡ßá‡¶® (simulated ‡¶¨‡¶æ real)

---

### 5. Footer - Endpoints Monitor

**‡¶Ø‡¶æ monitor ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º:**

| Endpoint | URL | Check Interval |
|----------|-----|----------------|
| Main Server | http://127.0.0.1:8155 | 5 seconds |
| Health | /health | 5 seconds |
| System Info | /system/info | 5 seconds |
| Models Installed | /models/installed | 5 seconds |
| Models Available | /models/available | 5 seconds |

**Status Indicators:**
- üü¢ Green dot + "Online" = Server responding
- üî¥ Red dot + "Offline" = Server not responding

**Interactive Features:**
- **Copy button:** URL clipboard ‡¶è copy ‡¶ï‡¶∞‡ßá
- **Status button:** Manual refresh ‡¶ï‡¶∞‡ßá status check
- **Toggle button:** Footer hide/show ‡¶ï‡¶∞‡ßá
- **Reset button:** Layout reset ‡¶ï‡¶∞‡ßá

---

## üîç API Endpoints ‡¶ü‡ßá‡¶∏‡ßç‡¶ü

### Manual Testing (cURL)

#### 1. Health Check
```bash
curl http://localhost:8155/health
```

**Expected:** `{"status":"healthy",...}`

#### 2. System Info
```bash
curl http://localhost:8155/system/info
```

**Expected:** CPU, RAM, GPU info

#### 3. Models Installed
```bash
curl http://localhost:8155/models/installed
```

**Expected:** List of installed models

#### 4. Models Available
```bash
curl http://localhost:8155/models/available
```

**Expected:** Recommended models for your tier

#### 5. Runtime Status
```bash
curl http://localhost:8155/runtime/status
```

**Expected:** Currently loaded models

---

## ü§ñ Model Testing

### Complete Model Test Flow

#### Step 1: Check Available Models
```bash
curl http://localhost:8155/models/installed
```

**Find a GGUF model (e.g., tinyllama-gguf)**

#### Step 2: Load Model
```bash
curl -X POST "http://localhost:8155/runtime/load/tinyllama-gguf?threads=4"
```

**Expected Response:**
```json
{
  "status": "ready",
  "model": "tinyllama-gguf",
  "port": 8080,
  "pid": 12345
}
```

**Wait:** 3-4 seconds

#### Step 3: Check Runtime Status
```bash
curl http://localhost:8155/runtime/status
```

**Expected:**
```json
{
  "models": [
    {
      "model": "tinyllama-gguf",
      "status": "ready",
      "port": 8080
    }
  ]
}
```

#### Step 4: Generate Text
```bash
curl -X POST http://localhost:8155/api/generate \
  -H "Content-Type: application/json" \
  -d "{\"model\":\"tinyllama-gguf\",\"prompt\":\"Write hello world Python\"}"
```

**Expected:** Model generated response (3-5 seconds)

#### Step 5: Unload Model
```bash
curl -X POST http://localhost:8155/runtime/unload/tinyllama-gguf
```

**Expected:**
```json
{
  "status": "stopped",
  "model": "tinyllama-gguf"
}
```

---

## üß™ Automated Tests

### Run Core Tests

```bash
cd C:\model\test
python run_core_tests.py
```

**Expected Output:**
```
[1/5] Testing: 01_preflight_check
‚úì PASS (0.9s)

[2/5] Testing: 02_model_lifecycle
‚úì PASS (6.5s)

[3/5] Testing: 03_api_standard_check
‚úì PASS (0.9s)

[4/5] Testing: 04_ui_data_integrity
‚úì PASS (1.1s)

[5/5] Testing: 05_integrated_session_test
‚úì PASS (7.2s)

Success Rate: 100.0%
```

---

### Run Individual Tests

```bash
# Preflight check
python test/01_preflight_check.py

# Model lifecycle
python test/02_model_lifecycle.py

# API standard check
python test/03_api_standard_check.py
```

---

### Integration Demo

```bash
cd C:\model\test
python demo_integration.py
```

**‡¶è‡¶á demo ‡¶¶‡ßá‡¶ñ‡¶æ‡¶¨‡ßá:**
- All services status
- AI text generation
- System integration examples

---

## üìä Expected Performance

### Load Times
- TinyLlama (Q2_K): **3-4 seconds**
- Phi-2 (Q4_K): **5-7 seconds**
- Larger models: **10-15 seconds**

### Generation Speed
- CPU (4 threads): **10-15 tokens/second**
- CPU (8 threads): **15-20 tokens/second**
- GPU accelerated: **100+ tokens/second**

### API Latency
- Health check: **<50ms**
- Model list: **<100ms**
- System info: **<100ms**
- Generate (64 tokens): **3-5 seconds**

---

## ‚ùå Common Issues & Solutions

### Issue 1: Server Not Responding
**Symptoms:** All endpoints showing offline

**Solution:**
```bash
# Check if server running
curl http://localhost:8155/health

# If not, start server
python model_server.py
```

---

### Issue 2: Model Not Found (404)
**Symptoms:** `/runtime/load/{model}` returns 404

**Solution:**
```bash
# Check installed models
curl http://localhost:8155/models/installed

# Download a GGUF model
curl -X POST http://localhost:8155/download/start \
  -d '{"model_name":"tinyllama-gguf","repo_id":"TheBloke/TinyLlama-1.1B-Chat-GGUF"}'
```

---

### Issue 3: Model Not Loaded (409)
**Symptoms:** `/api/generate` returns 409 Conflict

**Solution:**
```bash
# Load model first
curl -X POST http://localhost:8155/runtime/load/tinyllama-gguf
```

---

### Issue 4: Chat Not Working in UI
**Symptoms:** Simulated response only, no real AI

**Solution:**
1. Open `testui.html` in text editor
2. Find `sendMessage()` function
3. Uncomment "REAL API CALL" section
4. Comment out "SIMULATED RESPONSE" section
5. Make sure model is loaded
6. Refresh browser

---

## ‚úÖ Test Checklist

### Before Testing:
- [ ] Server running at port 8155
- [ ] At least one GGUF model installed
- [ ] llama.cpp runtime available

### Test UI Features:
- [ ] System info displays correctly
- [ ] Models list shows installed models
- [ ] Refresh button works
- [ ] Endpoints monitor shows status
- [ ] Clock updates every second
- [ ] Copy URL works
- [ ] Chat interface accepts input

### Test API with Real Model:
- [ ] Load model successfully
- [ ] Runtime status shows "ready"
- [ ] Generate text works
- [ ] Response time acceptable (<10s)
- [ ] Unload model successfully

### Test Automated Suite:
- [ ] All 5 core tests pass
- [ ] Duration under 30 seconds
- [ ] No errors in output
- [ ] JSON reports generated

---

## üìû Need Help?

### Check Logs:
```bash
# Server logs
cat C:\model\logs\server.log

# Runtime logs
cat C:\model\logs\runtime_tinyllama-gguf.log
```

### Debug Mode:
```bash
# Add to model_server.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Test Reports:
```bash
# Check test results
cat C:\model\test\core_test_report.json
```

---

## üéØ Quick Reference

### Start Testing (3 commands):
```bash
# 1. Start server
cd C:\model && python model_server.py

# 2. (New terminal) Load model
curl -X POST http://localhost:8155/runtime/load/tinyllama-gguf

# 3. Open test UI
start http://localhost:8080  # or file:///C:/model/test/testui.html
```

### Stop Testing:
```bash
# 1. Unload model
curl -X POST http://localhost:8155/runtime/unload/tinyllama-gguf

# 2. Stop server
# Press Ctrl+C in server terminal
```

---

## üìù Test Report Template

```markdown
# Test Session Report

Date: 2025-10-18
Tester: Your Name

## Environment
- OS: Windows 10
- RAM: 16 GB
- Model: tinyllama-gguf

## Tests Performed
- [ ] Server health check
- [ ] System info API
- [ ] Models list API
- [ ] Model load
- [ ] Text generation
- [ ] Model unload

## Results
‚úì All tests passed
Duration: 25 seconds

## Issues Found
None

## Performance
- Load time: 3.5s
- Generation: 14 tokens/sec
- API latency: <100ms
```

---

**‡¶∏‡¶´‡¶≤ Testing ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∂‡ßÅ‡¶≠‡¶ï‡¶æ‡¶Æ‡¶®‡¶æ!** üéâ

**Questions?** Contact: infi@zombiecoder.my.id

