# ZombieCoder Local AI - GGUF Integration Guide for Editor Extension

## üìã ‡¶∏‡¶æ‡¶∞‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™ (Summary)

‡¶è‡¶á document Editor/IDE extension (Cursor/VS Code) ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø integration ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßá‡•§ ZombieCoder Local AI Framework ‡¶è‡¶ñ‡¶® ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡¶≠‡¶æ‡¶¨‡ßá GGUF models support ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç HuggingFace Hub ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø download ‡¶ì run ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§

---

## üóÇÔ∏è ‡¶®‡¶§‡ßÅ‡¶® Files ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá

### 1. `/scripts/model_downloader.py`
**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø:** HuggingFace Hub ‡¶•‡ßá‡¶ï‡ßá GGUF models download ‡¶ï‡¶∞‡¶æ

**‡¶Æ‡ßÇ‡¶≤ Functions:**
```python
download_model(repo_id, filename, dest_dir, token) -> Dict
get_gguf_files(repo_id, token) -> list
```

**‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**
```python
from scripts.model_downloader import download_model

result = download_model(
    repo_id="TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
    filename="tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
    dest_dir="./models"
)
```

---

### 2. `/scripts/gguf_loader.py`
**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø:** GGUF models load ‡¶ï‡¶∞‡¶æ llama.cpp backend ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá

**‡¶Æ‡ßÇ‡¶≤ Functions:**
```python
load_model(model_path, port, threads, gpu_layers, context_size, background) -> Dict
check_model_status(port) -> Dict
find_llama_cpp_server() -> Optional[Path]
```

**‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**
```python
from scripts.gguf_loader import load_model

result = load_model(
    model_path="./models/tinyllama-gguf/tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
    port=8080,
    threads=4,
    gpu_layers=0
)
```

---

### 3. `/scripts/registry_manager.py`
**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø:** Model registry management (installed ‡¶è‡¶¨‡¶Ç available models track ‡¶ï‡¶∞‡¶æ)

**‡¶Æ‡ßÇ‡¶≤ Functions:**
```python
add_installed_model(model_name, repo_id, filename, location, size_mb, format)
remove_installed_model(model_name)
list_installed_models() -> Dict
validate_model_format(model_path) -> Dict
```

---

### 4. `/models/model_registry.json`
**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø:** ‡¶∏‡¶¨ models ‡¶è‡¶∞ centralized registry

**Structure:**
```json
{
  "version": "1.0",
  "last_updated": "2025-10-18T03:00:00Z",
  "models": {
    "model-name": {
      "repo_id": "TheBloke/Model-GGUF",
      "filename": "model.Q4_K_M.gguf",
      "format": "gguf",
      "status": "installed",
      "location": "./models/model-name/model.Q4_K_M.gguf",
      "size_mb": 1500,
      "installed_at": "2025-10-18T03:00:00Z"
    }
  },
  "available": {
    "model-name": {
      "repo_id": "TheBloke/Model-GGUF",
      "filename": "model.Q4_K_M.gguf",
      "format": "gguf",
      "status": "available",
      "size_estimate_mb": 1500,
      "description": "Model description"
    }
  }
}
```

---

## üîå API Endpoints (Integration Points)

### **Download Endpoints**

#### 1. POST `/download/start`
Model download ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßá

**Request Body:**
```json
{
  "model_name": "phi-2-gguf",
  "repo_id": "TheBloke/phi-2-GGUF",
  "revision": null
}
```

**Response:**
```json
{
  "status": "started",
  "model_name": "phi-2-gguf",
  "repo_id": "TheBloke/phi-2-GGUF",
  "target_dir": "C:\\model\\models\\phi-2-gguf"
}
```

---

#### 2. GET `/download/status/{model}`
Download status check ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç complete ‡¶π‡¶≤‡ßá format validation ‡¶ï‡¶∞‡ßá

**Response (Completed):**
```json
{
  "status": "completed",
  "model": "phi-2-gguf",
  "format_validation": {
    "format": "gguf",
    "valid": true,
    "files": ["phi-2.Q4_K_M.gguf"],
    "message": "GGUF format detected - compatible with llama.cpp"
  },
  "registry_updated": true
}
```

**Response (Invalid Format):**
```json
{
  "status": "completed",
  "model": "phi-2",
  "format_validation": {
    "format": "safetensors",
    "valid": false,
    "files": ["model.safetensors"],
    "message": "SafeTensors format detected - NOT compatible with current runtime. Please download GGUF version."
  }
}
```

---

### **Registry Endpoints**

#### 3. GET `/registry/models`
‡¶∏‡¶¨ installed ‡¶è‡¶¨‡¶Ç available models list ‡¶ï‡¶∞‡ßá

**Response:**
```json
{
  "installed": {
    "tinyllama-gguf": {
      "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
      "filename": "tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
      "format": "gguf",
      "status": "installed",
      "location": "./models/tinyllama-gguf/tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
      "size_mb": 641.5,
      "installed_at": "2025-10-18T03:00:00Z"
    }
  },
  "available": {
    "phi-2-gguf": {
      "repo_id": "TheBloke/phi-2-GGUF",
      "filename": "phi-2.Q4_K_M.gguf",
      "format": "gguf",
      "status": "available",
      "size_estimate_mb": 1500,
      "description": "Phi-2 2.7B GGUF Q4_K_M quantization"
    }
  },
  "count_installed": 1,
  "count_available": 3
}
```

---

#### 4. GET `/registry/validate/{model}`
Model format validate ‡¶ï‡¶∞‡ßá

**Response (GGUF - Valid):**
```json
{
  "format": "gguf",
  "valid": true,
  "files": ["model.Q4_K_M.gguf"],
  "message": "GGUF format detected - compatible with llama.cpp"
}
```

**Response (SafeTensors - Invalid):**
```json
{
  "format": "safetensors",
  "valid": false,
  "files": ["model.safetensors"],
  "message": "SafeTensors format detected - NOT compatible with current runtime. Please download GGUF version."
}
```

---

### **Runtime Endpoints**

#### 5. POST `/runtime/load/{model}`
Model load ‡¶ï‡¶∞‡ßá (‡¶∂‡ßÅ‡¶ß‡ßÅ GGUF support ‡¶ï‡¶∞‡ßá)

**Query Parameters:**
- `threads` (optional, default: 4)

**Response:**
```json
{
  "status": "ready",
  "model": "tinyllama-gguf",
  "port": 8080,
  "pid": 12345,
  "command": "C:\\model\\config\\llama.cpp\\server.exe --model ... --port 8080 --threads 4"
}
```

**Error Response (Non-GGUF):**
```json
{
  "detail": "GGUF weights not found under C:\\model\\models\\phi-2"
}
```

---

## ‚öôÔ∏è Integration Workflow

### **Complete Download ‚Üí Validate ‚Üí Load Flow**

```typescript
// 1. Start Download
const downloadResp = await fetch('http://localhost:8155/download/start', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model_name: 'phi-2-gguf',
    repo_id: 'TheBloke/phi-2-GGUF'
  })
});

// 2. Poll Download Status
const checkStatus = async (modelName: string) => {
  const resp = await fetch(`http://localhost:8155/download/status/${modelName}`);
  const status = await resp.json();
  
  if (status.status === 'completed') {
    // 3. Check Format Validation
    if (status.format_validation?.valid) {
      console.log('‚úÖ GGUF format - Ready to load');
      return true;
    } else {
      console.warn('‚ùå Invalid format:', status.format_validation?.message);
      return false;
    }
  }
  
  // Continue polling
  return null;
};

// 4. Load Model (if GGUF)
const loadResp = await fetch('http://localhost:8155/runtime/load/phi-2-gguf?threads=4', {
  method: 'POST'
});
const loadResult = await loadResp.json();

if (loadResult.status === 'ready') {
  console.log('‚úÖ Model loaded on port', loadResult.port);
}
```

---

## üö® Important Rules for Editor Extension

### ‚úÖ ‡¶ï‡¶∞‡¶£‡ßÄ‡¶Ø‡¶º (DO's)

1. **GGUF-Only Policy:**
   - ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ GGUF models download ‡¶è‡¶¨‡¶Ç load ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá
   - `/registry/validate/{model}` ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá format check ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá
   - Invalid format ‡¶π‡¶≤‡ßá user ‡¶ï‡ßá warning ‡¶¶‡ßá‡¶ñ‡¶æ‡¶§‡ßá ‡¶π‡¶¨‡ßá

2. **Download Completion:**
   - Download complete ‡¶π‡¶≤‡ßá automatically registry update ‡¶π‡¶¨‡ßá
   - `/download/status/{model}` ‡¶•‡ßá‡¶ï‡ßá `format_validation` check ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá

3. **Registry Sync:**
   - Model install/uninstall ‡¶π‡¶≤‡ßá registry automatically update ‡¶π‡¶¨‡ßá
   - `/registry/models` endpoint use ‡¶ï‡¶∞‡ßá latest state ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶Ø‡¶æ‡¶¨‡ßá

4. **Error Handling:**
   - SafeTensors models load ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶≤‡ßá clear error message ‡¶¶‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá
   - User ‡¶ï‡ßá GGUF version download ‡¶ï‡¶∞‡¶§‡ßá suggest ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá

### ‚ùå ‡¶ï‡¶∞‡¶£‡ßÄ‡¶Ø‡¶º ‡¶®‡¶Ø‡¶º (DON'Ts)

1. **SafeTensors/PyTorch Models:**
   - SafeTensors ‡¶¨‡¶æ PyTorch models load ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ
   - ‡¶è‡¶ó‡ßÅ‡¶≤‡ßã current runtime (llama.cpp) support ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ

2. **Manual Registry Edit:**
   - `model_registry.json` manually edit ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ
   - API endpoints ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®

3. **Skip Format Validation:**
   - Download complete ‡¶π‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡¶∞‡ßá format validation skip ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ
   - ‡¶è‡¶ü‡¶æ guarantee ‡¶ï‡¶∞‡ßá ‡¶Ø‡ßá model compatible

---

## üì¶ Recommended GGUF Models

### Entry Level (2-3 GB RAM)
```json
{
  "tinyllama-1.1b-gguf": {
    "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
    "filename": "tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
    "size_mb": 641
  }
}
```

### Mid Range (4-6 GB RAM)
```json
{
  "phi-2-gguf": {
    "repo_id": "TheBloke/phi-2-GGUF",
    "filename": "phi-2.Q4_K_M.gguf",
    "size_mb": 1500
  },
  "llama-3.2-3b-gguf": {
    "repo_id": "bartowski/Llama-3.2-3B-Instruct-GGUF",
    "filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    "size_mb": 2000
  }
}
```

---

## üß™ Testing Commands

### Test Model Downloader
```bash
python scripts/model_downloader.py
```

### Test GGUF Loader
```bash
python scripts/gguf_loader.py "./models/tinyllama-gguf/tinyllama-1.1b-chat-v1.0.Q2_K.gguf" 8080 4
```

### Test Registry Manager
```python
from scripts.registry_manager import ModelRegistry

registry = ModelRegistry("models/model_registry.json")
print(registry.list_installed_models())
```

### Test API Endpoints
```bash
# Validate format
curl http://localhost:8155/registry/validate/tinyllama-gguf

# Get all models
curl http://localhost:8155/registry/models
```

---

## üìù Notes for Extension Developer

1. **Server Base URL:** `http://localhost:8155`
2. **Default Model Port:** `8080` (auto-incremented for multiple models)
3. **llama.cpp Location:** `C:\model\config\llama.cpp\server.exe`
4. **Models Directory:** `C:\model\models\`
5. **Registry File:** `C:\model\models\model_registry.json`

---

## üéØ Summary Checklist

- ‚úÖ GGUF models download ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º HuggingFace ‡¶•‡ßá‡¶ï‡ßá
- ‚úÖ Format validation automatic ‡¶π‡¶Ø‡¶º download complete ‡¶è‡¶∞ ‡¶™‡¶∞‡ßá
- ‚úÖ Registry automatic update ‡¶π‡¶Ø‡¶º install/uninstall ‡¶è
- ‚úÖ SafeTensors models detect ‡¶ï‡¶∞‡ßá warning ‡¶¶‡ßá‡¶Ø‡¶º
- ‚úÖ ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ GGUF models load ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá llama.cpp ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá
- ‚úÖ ‡¶∏‡¶¨ API endpoints documented ‡¶è‡¶¨‡¶Ç tested

---

**Integration Complete! üöÄ**

Editor ‡¶≠‡¶æ‡¶á ‡¶è‡¶ñ‡¶® ‡¶è‡¶á endpoints ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá seamless GGUF model management implement ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶®‡•§
